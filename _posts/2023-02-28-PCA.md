---
title: Principal Component Analysis
date: 2023-02-28 18:27:10
read: 3
---

*This post was first written on [medium](https://medium.com/@yahya.sekinat/principal-component-analysis-pca-the-math-intuition-behind-it-4ea1b7bfbcfa)*

PCA is a dimensionality reduction technique used in data mining to get the most important features that best describe a data set. Because it can reduce the dimensions (i.e. features of the data), it aids in visualizing the behavior of the data.

### The Math
PCA is a statistical interpretation of singular value decomposition (SVD). Its hugely reliant on the correlation between the dimensions of our data, more specifically the covariance matrix.

### PCA is also a linear transformation.
The principal components (PCs) are linear combinations of the original features of the data. PCA is deeply dependent on the correlation between the features of a dataset.

To perform the linear transformation that will produce the eigs(the eigen values and their corresponding eigen vectors), we need to compute the covariant matrix.
To do this:

1. Compute matrix B which is the mean-subtracted data <br>
$B = x -  \overline{x}$
2. Create the covariant matrix which is a square matrix with dimension = the dimension of the original data. <br>
$\begin{bmatrix} var(x)&cov(x,y) \cr cov(x,y)& var(y)\end{bmatrix}$

For the purpose of this piece, we take our covariant matrix to be:
 $\begin{bmatrix}9&4\cr4&3\end{bmatrix}$ 

3. Perform linear transformation
<figure><img src="/sources/posts_imgs/lin_trans.png " alt="linear transformation" style="width:100%"><figcaption align = "center">using the covariance matrix, the spread of the data is transformed from the circle to the ellips</figcaption></figure>
```
  (x,y) ===> (9x + 4y, 4x + 3y)
  (0,0) ===> (0,0)
  (1,0) ===> (9,4)
  (0,1) ===> (4,3)
  (-1,0) ===> (-9,-4)
  (0,-1) ===> (-4,-3)
```

4. Compute the eigen vectors and the eigen values of the covariant matrix. <br>
$\begin{bmatrix}9&4\cr4&3\end{bmatrix} * v = \lambda * v$ 

$v$ = the eigen vector<br>
$\lambda$ = the eigen value


5. The principal components are computed by multiplying the mean-subtracted data by the eigen vector of the covariant matrix <br>
$T = BV$<br>
$T$ = the principal components

6. T can also be calculated simply using SVD.<br>
$SVD(B) = U\*\Sigma\*V^T$ <br>
$T = BV = U\Sigma$

$ U = V $ = eigen vectors<br>
$B = \Sigma $ = eigen values


### PCA WRT a Dataset

1. Say we have a data set with 5 dimensions.<br>
>| x1 | x2 | x3 | x4 | x5 |
| --- | ---| ---| ---|---|
| 0 | 8 | 2 | 0 | 1 |
| 3 | 5 | 1 | 8 | 9 |
| 3 | 2 | 0 | 7 | 0 |
| 3 | 3 | 0 | 2 | 6 |
| 6 | 6 | 1 | 7 | 9 |
| 5 | 6 | 1 | 8 | 0 |


```python
import numpy as np
B = np.array([[0, 8, 2, 0, 1],
                [3, 5, 1, 8, 9],
                [3, 2, 0, 7, 0],
                [3, 3, 0, 2, 6],
                [6, 6, 1, 7, 9],
                [5, 6, 1, 8 0]])
```

2. Compute the covariance matrix of this dataset whose dimension is (5x5),<br>
![5D plot](/sources/posts_imgs/5D_plot.png)

3. Compute the eigs of this matrix (in this case weâ€™ll have 5 pairs of eigen vectors and values),<br>
$v_{1}, \lambda_{1}$<br>
$v_{2}, \lambda_{2}$<br>
$v_{3}, \lambda_{3}$<br>
$v_{4}, \lambda_{4}$<br>
$v_{5}, \lambda_{5}$

4. Rank the eigs based on the magnitude of the eigen values and pick the most important (the highest in magnitude) sets of eigs.<br>
![ranked](/sources/posts_imgs/rank_eigs.png)

5. Choose the most important based on the eigen values ranked in descending order. For example, choosing the highest 2 enables us to visualize the data in a 2D plane.<br>
![2D plot](/sources/posts_imgs/2D_plot.png)







### PCR

PCR = PCA + a linear regression step that uses the method of least squares.

Usually in this linear regression step, you predict the value of a set target feature from the outcome of your PCs i.e. This target feature is not included in your feature sets at the time of the PCA analysis.