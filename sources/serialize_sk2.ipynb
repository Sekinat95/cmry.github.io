{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Pipeline Persistence and JSON Serialization Part II\n",
    "\n",
    "    By Chris Emmery, 14-04-2016, 5 minute read\n",
    "---\n",
    "\n",
    "*This is a follow-up to [this](./serialize) post.*\n",
    "\n",
    "In my last entry, I wrote about several hurdles on the way to replacing pickle\n",
    "with JSON for storing scikit-learn pipelines. While my previous solution was\n",
    "satisfactory for handling a class per file, storing an entire pipeline\n",
    "introduces more complexity than I previously assumed. In this follow-up, I will\n",
    "quickly illustrate one of these issues, and provide an effective solution.\n",
    "\n",
    "> Please note that all code is in Python 3.x, sklearn 0.17, and numpy 1.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Recap\n",
    "\n",
    "We left off using `__dict__` representations for each of the `scikit-learn`\n",
    "classes, converting their data structures (including those from `numpy`) with\n",
    "a small script and storing them per pipeline item. This would make a final\n",
    "application look as follows:\n",
    "\n",
    "```python\n",
    "vec = deserialize(DictVectorizer(), json.load(open('vec.json')))\n",
    "svm = deserialize(LinearSVC(), json.load(open('clf.json')))\n",
    "\n",
    "user_input = \"hey can I get more examples\"\n",
    "\n",
    "grams = vec.transform(extract_grams(user_input, [2]))\n",
    "print(svm.predict(grams))\n",
    "\n",
    "# output ---------\n",
    "\n",
    "[1]\n",
    "```\n",
    "\n",
    "The assumptions are that 1) your pipeline is quite small, so it's not too\n",
    "convoluted to store their items seperately, and 2) it has a static components,\n",
    "e.g. it will always use an SVM, and not do any preprocessing. If you're\n",
    "interested in reproducibility only, this is good enough. For demos, however,\n",
    "flexibility can be important.\n",
    "\n",
    "## Drop-in Models\n",
    "Let's say we want to *just* allow selection of a trained model. The easiest\n",
    "way would be to store the pipeline in a dictionary, for example:\n",
    "\n",
    "```python\n",
    "pipeline = {\n",
    "    \"clf\": GaussianNB(),\n",
    "    \"vec\": DictVectorizer(),\n",
    "}\n",
    "```\n",
    "\n",
    "It shouldn't really matter what `clf` is, as long as it has the same\n",
    "methods as all other `sklearn` classes. Subsequently, our application can be\n",
    "reduced to the following:\n",
    "\n",
    "```python\n",
    "pl = deserialize(Pipeline(), json.load(open('pipeline.json')))\n",
    "\n",
    "user_input = \"hey can I get more examples\"\n",
    "grams = pl['vec'].transform(extract_grams(user_input, [2]))\n",
    "print(pl['clf'].predict(grams))\n",
    "```\n",
    "\n",
    "However, to achieve this, we would need to serialize the classes in a way that\n",
    "we can deserialize them to their initialized form. Hence, just storing them as\n",
    "their `__dict__` representation is not enough.\n",
    "\n",
    "## Problem: Serializing Python Objects\n",
    "\n",
    "How does one store a python object in a form that JSON can handle, and we can\n",
    "deserialize in our application? Remeber that before, we set classes like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import serialize_sk as sr\n",
    "\n",
    "def deserialize(class_init, attr):\n",
    "    for k, v in attr.items():\n",
    "        setattr(class_init, k, sr.json_to_data(v))\n",
    "    return class_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to set the attributes (with `__dict__`), but we need a way\n",
    "to get a representation from a class object which we can use to initalize it.\n",
    "Python allows you to get a string name with `__class__`, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.dict_vectorizer.DictVectorizer'>\n",
      "DictVectorizer\n",
      "sklearn.feature_extraction.dict_vectorizer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer()\n",
    "print(str(vec.__class__))\n",
    "print(vec.__class__.__name__)\n",
    "print(vec.__module__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output, the first returns a class object, and the second\n",
    "its name. However, we would need the full path in order to import it, which\n",
    "leaves us with the third solution. From there, we could easily import and\n",
    "initialize it by string, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "class_ = getattr(sys.modules[vec.__module__], vec.__class__.__name__)\n",
    "\n",
    "new_vec = class_()\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we can use `setattr` again like in the `deserialize` function above to\n",
    "return our settings. Just need to store them both in a format along with the\n",
    "`__dict__` to pass to the deserializer. Something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def serialize_class(cls_):\n",
    "    return sr.data_to_json({'mod': cls_.__module__, 'name': cls_.__class__.__name__,\n",
    "            'attr': cls_.__dict__})\n",
    "\n",
    "def deserialize_class(cls_repr):\n",
    "    cls_repr = sr.json_to_data(cls_repr)\n",
    "    cls_ = getattr(sys.modules[cls_repr['mod']], cls_repr['name'])\n",
    "    cls_init = cls_()\n",
    "    for k, v in cls_repr['attr'].items():\n",
    "        setattr(cls_init, k, v)\n",
    "    return cls_init\n",
    "\n",
    "cls_str = serialize_class(vec)\n",
    "json.dump(cls_str, open('./vec_class.json', 'w'))\n",
    "\n",
    "cls_js = json.load(open('./vec_class.json'))\n",
    "deserialize_class(cls_js)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the classes can be used in a pipeline dictionary.\n",
    "As the [script](https://github.com/cmry/cmry.github.io/blob/master/sources/serialize_sk.py) I provided\n",
    "in the previous post is recursive, these methods can be built in without much\n",
    "effort. However, while reading into these object serialization techniques I\n",
    "found an even better alternative (given that you don't mind dependencies).\n",
    "\n",
    "## Conclusion and Package\n",
    "\n",
    "So far I managed to manually convert most `numpy` cases in scikit-learn's\n",
    "modules. And the modules themselves to be stored in dictionaries for\n",
    "flexibility. However, I decided to sweep all of this off the table for\n",
    "[jsonpickle](https://github.com/jsonpickle/jsonpickle). This package covers a\n",
    "*lot* more edge-cases with a way more extensive implementation. Quick\n",
    "demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"separator\": \"=\", \"py/object\": \"sklearn.feature_extraction.dict_vectorizer.DictVectorizer\", \"sparse\": true, \"sort\": true, \"dtype\": {\"py/type\": \"numpy.float64\"}}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonpickle\n",
    "\n",
    "vec_repr = jsonpickle.encode(vec)\n",
    "vec_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with a quick `decode` we're back to our old python storage format!\n",
    "\n",
    "That's it\n",
    "for now, if I encounter any more challenges there will be another follow-up. As\n",
    "before, I've written this up in a Jupyter [notebook](https://github.com/cmry/cmry.github.io/blob/master/sources/serialize_sk2.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
